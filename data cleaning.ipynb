{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dab45ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import io\n",
    "import requests\n",
    "import zipfile\n",
    "from sklearn.cluster import DBSCAN\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Load directly from GitHub\n",
    "url = \"https://github.com/clarabot1/LA28_bus_network/raw/main/airbnb_listings.csv.zip\"  # Updated URL\n",
    "\n",
    "# Download the zip file\n",
    "response = requests.get(url)\n",
    "response.raise_for_status()  # Check for download errors\n",
    "\n",
    "# Extract the CSV file from the zip file\n",
    "with zipfile.ZipFile(io.BytesIO(response.content)) as zip_ref:\n",
    "    with zip_ref.open(\"airbnb_listings.csv\") as csv_file:\n",
    "        df = pd.read_csv(csv_file)\n",
    "\n",
    "# Show the dataframe\n",
    "df.head()\n",
    "airbnb_listings = df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d4a305",
   "metadata": {},
   "source": [
    "# Data Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf277a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "airbnb_listings.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62444de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep colums that are id, accommodates, latitude, longitude\n",
    "airbnb_listings = airbnb_listings[[\"id\", \"accommodates\", \"latitude\", \"longitude\"]]\n",
    "# remove duplicates\n",
    "airbnb_listings = airbnb_listings.drop_duplicates()\n",
    "# remove rows with null values\n",
    "airbnb_listings = airbnb_listings.dropna()\n",
    "# remove rows with 0 values\n",
    "airbnb_listings = airbnb_listings[airbnb_listings[\"accommodates\"] > 0]\n",
    "airbnb_listings.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ecef050",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load directly from GitHub\n",
    "url = \"https://raw.githubusercontent.com/gr-oll/SLO_LA_Olympics/refs/heads/main/datasets/hotels.csv\"\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "# Show the dataframe\n",
    "df.head()\n",
    "\n",
    "hotels = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5300f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep only the relevant columns\n",
    "hotels = hotels[[\"id\", \"people\", \"lon\", \"lat\"]]\n",
    "# already handled duplicates and null values\n",
    "hotels.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b01877f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#accommodates summary statistics\n",
    "display(airbnb_listings[\"accommodates\"].describe())\n",
    "#people summary statistics\n",
    "hotels[\"people\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f42735",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Align column names for merging\n",
    "hotels_renamed = hotels.rename(columns={\"lat\": \"latitude\", \"lon\": \"longitude\", \"people\": \"accommodates\"})\n",
    "\n",
    "hotels_renamed[\"id\"] = \"H\" + hotels_renamed[\"id\"].astype(str)\n",
    "airbnb_listings[\"id\"] = \"A\" + airbnb_listings[\"id\"].astype(str)\n",
    "\n",
    "# Merge the dataframes vertically\n",
    "merged_df = pd.concat([airbnb_listings, hotels_renamed], ignore_index=True)\n",
    "\n",
    "# Display the merged dataframe\n",
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997388a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#count the number of listings\n",
    "len(merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67fbc5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sum of accommodates\n",
    "merged_df[\"accommodates\"].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3a0c4a",
   "metadata": {},
   "source": [
    "# Create clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ccc9a61",
   "metadata": {},
   "source": [
    "## OG Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5859a74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Define the number of clusters (you can adjust this based on your requirements)\n",
    "num_clusters = 35\n",
    "\n",
    "# Prepare the data for clustering (latitude, longitude, accommodates)\n",
    "features_for_clustering = merged_df[[\"latitude\", \"longitude\", \"accommodates\"]].copy()\n",
    "\n",
    "# Normalize the accommodates column to balance its scale with latitude and longitude\n",
    "scaler = MinMaxScaler()\n",
    "features_for_clustering[\"accommodates\"] = scaler.fit_transform(features_for_clustering[[\"accommodates\"]])\n",
    "\n",
    "# Perform KMeans clustering\n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "merged_df[\"kmeans_cluster\"] = kmeans.fit_predict(features_for_clustering)\n",
    "\n",
    "# Analyze the clusters\n",
    "cluster_summary = merged_df.groupby(\"kmeans_cluster\").agg(\n",
    "    total_accommodates=(\"accommodates\", \"sum\"),\n",
    "    count=(\"id\", \"count\"),\n",
    "    avg_latitude=(\"latitude\", \"mean\"),\n",
    "    avg_longitude=(\"longitude\", \"mean\")\n",
    ")\n",
    "\n",
    "# Display the cluster summary\n",
    "cluster_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5a5023",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882ececa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a color palette for the clusters\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.colors as colors\n",
    "import folium\n",
    "\n",
    "# Generate a color map with distinct colors for each cluster\n",
    "num_clusters = merged_df[\"kmeans_cluster\"].nunique()\n",
    "colormap = cm.get_cmap(\"tab20\", num_clusters)\n",
    "cluster_colors = [colors.rgb2hex(colormap(i)[:3]) for i in range(num_clusters)]\n",
    "\n",
    "# Create a folium map centered around Los Angeles\n",
    "m = folium.Map(location=[34.0522, -118.2437], zoom_start=12)\n",
    "\n",
    "# Map cluster IDs to integer indices\n",
    "cluster_id_to_index = {cluster_id: idx for idx, cluster_id in enumerate(merged_df[\"kmeans_cluster\"].unique())}\n",
    "\n",
    "# Add points to the map with colors based on their cluster\n",
    "for _, row in merged_df.iterrows():\n",
    "    cluster_index = cluster_id_to_index[row[\"kmeans_cluster\"]]\n",
    "    cluster_color = cluster_colors[cluster_index]\n",
    "    folium.CircleMarker(\n",
    "        location=[row[\"latitude\"], row[\"longitude\"]],\n",
    "        radius=5,\n",
    "        color=cluster_color,\n",
    "        fill=True,\n",
    "        fill_color=cluster_color,\n",
    "        fill_opacity=0.7,\n",
    "        popup=f\"ID: {row['id']}, Accommodates: {row['accommodates']}, Cluster: {row['kmeans_cluster']}\"\n",
    "    ).add_to(m)\n",
    "\n",
    "\n",
    "# Display the map\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ada250",
   "metadata": {},
   "source": [
    "## Break Down the Big Ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246de723",
   "metadata": {},
   "outputs": [],
   "source": [
    "def subcluster_large_cluster(cluster_id, n_subclusters, df):\n",
    "    cluster_df = df[df[\"kmeans_cluster\"] == cluster_id]\n",
    "    kmeans_sub = KMeans(n_clusters=n_subclusters, random_state=0, n_init=10)\n",
    "    cluster_df[\"sub_cluster\"] = kmeans_sub.fit_predict(cluster_df[[\"latitude\", \"longitude\"]])\n",
    "    df.loc[df[\"kmeans_cluster\"] == cluster_id, \"sub_cluster\"] = (\n",
    "        str(cluster_id) + \"_\" + cluster_df[\"sub_cluster\"].astype(str)\n",
    "    )\n",
    "    return df\n",
    "\n",
    "# Subcluster specified clusters\n",
    "for cluster_id, n_subclusters in [(0, 4), (9, 6), (17, 3), (21, 4), (25, 4), (29, 2)]:\n",
    "    if cluster_id in merged_df[\"kmeans_cluster\"].unique():\n",
    "        merged_df = subcluster_large_cluster(cluster_id, n_subclusters, merged_df)\n",
    "    else:\n",
    "        print(f\"Cluster {cluster_id} not found.\")\n",
    "\n",
    "\n",
    "merged_df[\"sub_cluster\"] = merged_df[\"sub_cluster\"].fillna(merged_df[\"kmeans_cluster\"].astype(str))\n",
    "# Print the first few rows to verify\n",
    "print(merged_df.head())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63118ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31cf7db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop cluster 10 - avalon \n",
    "merged_df = merged_df[merged_df[\"kmeans_cluster\"] != 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acee2297",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the clusters\n",
    "cluster_summary = merged_df.groupby(\"sub_cluster\").agg(\n",
    "    total_accommodates=(\"accommodates\", \"sum\"),\n",
    "    count=(\"id\", \"count\"),\n",
    "    avg_latitude=(\"latitude\", \"mean\"),\n",
    "    avg_longitude=(\"longitude\", \"mean\")\n",
    ")\n",
    "\n",
    "# Display the cluster summary\n",
    "cluster_summary.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.cm as cm\n",
    "import matplotlib.colors as colors\n",
    "import folium\n",
    "\n",
    "# Use a continuous colormap like 'rainbow' to ensure each cluster has a unique color\n",
    "num_clusters = merged_df[\"sub_cluster\"].nunique()\n",
    "colormap = cm.get_cmap(\"rainbow\", num_clusters)\n",
    "cluster_colors = [colors.rgb2hex(colormap(i)[:3]) for i in range(num_clusters)]\n",
    "\n",
    "m = folium.Map(location=[34.0522, -118.2437], zoom_start=12)\n",
    "cluster_id_to_index = {cluster_id: idx for idx, cluster_id in enumerate(merged_df[\"sub_cluster\"].unique())}\n",
    "\n",
    "for _, row in merged_df.iterrows():\n",
    "    cluster_index = cluster_id_to_index[row[\"sub_cluster\"]]\n",
    "    cluster_color = cluster_colors[cluster_index]\n",
    "    folium.CircleMarker(\n",
    "        location=[row[\"latitude\"], row[\"longitude\"]],\n",
    "        radius=5,\n",
    "        color=cluster_color,\n",
    "        fill=True,\n",
    "        fill_color=cluster_color,\n",
    "        fill_opacity=0.7,\n",
    "        popup=f\"ID: {row['id']}, Accommodates: {row['accommodates']}, Cluster: {row['sub_cluster']}\"\n",
    "    ).add_to(m)\n",
    "for cluster_id, row in cluster_summary.iterrows():\n",
    "    folium.Marker(\n",
    "        location=[row['avg_latitude'], row['avg_longitude']],\n",
    "        popup=f\"Cluster: {cluster_id}<br>Total Accommodates: {row['total_accommodates']}<br>Count: {row['count']}\",\n",
    "        icon=folium.Icon(color=\"blue\")\n",
    "    ).add_to(m)\n",
    "m\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a06399",
   "metadata": {},
   "source": [
    "# Accomodations Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a1df4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a map centered around Los Angeles\n",
    "map_center = [34.0522, -118.2437]\n",
    "cluster_map = folium.Map(location=map_center, zoom_start=10)\n",
    "\n",
    "# Add markers for each cluster\n",
    "for cluster_id, row in cluster_summary.iterrows():\n",
    "    folium.Marker(\n",
    "        location=[row['avg_latitude'], row['avg_longitude']],\n",
    "        popup=f\"Cluster: {cluster_id}<br>Total Accommodates: {row['total_accommodates']}<br>Count: {row['count']}\",\n",
    "        icon=folium.Icon(color=\"blue\")\n",
    "    ).add_to(cluster_map)\n",
    "\n",
    "# Display the map\n",
    "cluster_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6fc66e8",
   "metadata": {},
   "source": [
    "# Bus Divisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da4afc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Depot\n",
    "\n",
    "####Depot dataframe and map\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load directly from GitHub\n",
    "url = \"https://raw.githubusercontent.com/clarabot1/LA28_bus_network/refs/heads/main/bus_divisions_cleaned-2.csv\"\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "# Show the dataframe\n",
    "df.head()\n",
    "\n",
    "bus_divisions = df\n",
    "\n",
    "# prompt: print the different values in FACILITY column, and drop all lines that are either TERMINAL and LOCATIONS\n",
    "\n",
    "print(df['FACILITY'].unique())\n",
    "\n",
    "\n",
    "bus_divisions = bus_divisions[~bus_divisions['FACILITY'].str.contains('LOCATION', na=False)]\n",
    "bus_divisions = bus_divisions[~bus_divisions['FACILITY'].str.contains('TERMINAL', na=False)]\n",
    "\n",
    "\n",
    "bus_divisions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8e1ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# prompt: #drop rows with NaN in address column in the bus_divisions dataframe, as well as rows with the following 4Digits: 4,6,10,11,20,21,22\n",
    "\n",
    "# Drop rows with NaN in 'address' column\n",
    "bus_divisions.dropna(subset=['Address'], inplace=True)\n",
    "\n",
    "# Define the 4Digits to exclude\n",
    "four_digits_to_exclude = [4, 6, 10, 11, 12, 20, 21, 22]\n",
    "\n",
    "# Convert '4Digits' column to numeric, coercing errors to NaN\n",
    "bus_divisions['4Digits'] = pd.to_numeric(bus_divisions['4Digits'], errors='coerce')\n",
    "\n",
    "# Drop rows where '4Digits' is in the list of values to exclude\n",
    "bus_divisions = bus_divisions[~bus_divisions['4Digits'].isin(four_digits_to_exclude)]\n",
    "\n",
    "#Now you can work with the modified 'bus_divisions' DataFrame\n",
    "bus_divisions\n",
    "\n",
    "\n",
    "\n",
    "print(bus_divisions[bus_divisions.isna().any(axis=1)])\n",
    "\n",
    "\n",
    "\n",
    "# Create a map centered around Los Angeles\n",
    "map_center = [34.0522, -118.2437]\n",
    "bus_division_map = folium.Map(location=map_center, zoom_start=10)\n",
    "\n",
    "# Add markers for each bus division location\n",
    "for index, row in bus_divisions.iterrows():\n",
    "    folium.Marker(\n",
    "        location=[row['Latitude'], row['Longitude']],\n",
    "        popup=f\"Bus Division: {row['NAME']}\",  # Assuming 'division_name' column exists\n",
    "        icon=folium.Icon(color=\"green\")\n",
    "    ).add_to(bus_division_map)\n",
    "\n",
    "bus_division_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c54a9c",
   "metadata": {},
   "source": [
    "#### Divisions capacity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b41fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load directly from GitHub\n",
    "url = \"https://raw.githubusercontent.com/clarabot1/LA28_bus_network/refs/heads/main/bus_divisions_cleaned_capacity.csv\"\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "# Show the dataframe\n",
    "df.head()\n",
    "\n",
    "depot_capacity = df\n",
    "depot_capacity = depot_capacity.rename(columns={'Amonut': 'amount'})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a1b05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# prompt: compute total capacity of each depot, and add the corresponding latitude and longitude from bus_divisions based on the matching 4Digits column\n",
    "\n",
    "# Merge depot capacity with bus division data based on the '4Digits' column\n",
    "merged_depot_data = pd.merge(depot_capacity, bus_divisions, on='4Digits', how='left')\n",
    "\n",
    "# Calculate the total capacity for each depot\n",
    "merged_depot_data['Total Capacity'] = merged_depot_data.groupby('4Digits')['amount'].transform('sum')\n",
    "\n",
    "# Select relevant columns and display the result\n",
    "# The 'Latitude' and 'Longitude' columns were likely renamed during the merge,\n",
    "# so we need to use their new names.\n",
    "depot_capacity_with_coordinates = merged_depot_data[['4Digits', 'Total Capacity', 'Latitude_y', 'Longitude_y']].drop_duplicates()\n",
    "depot_capacity_with_coordinates = depot_capacity_with_coordinates.rename(columns={'Latitude_y': 'Latitude', 'Longitude_y': 'Longitude'})\n",
    "depot_capacity_with_coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346484b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum total capacity\n",
    "total_capacity = depot_capacity_with_coordinates['Total Capacity'].sum()\n",
    "total_capacity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148abc66",
   "metadata": {},
   "source": [
    "# Olympic venues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a405e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "####Dataframe and map\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load directly from GitHub\n",
    "url = \"https://raw.githubusercontent.com/clarabot1/LA28_bus_network/refs/heads/main/Venue_data-4.csv\"\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "# Show the dataframe\n",
    "df.head()\n",
    "\n",
    "venues_locations = df\n",
    "\n",
    "venues_locations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d19d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "map_center = [34.0522, -118.2437]\n",
    "la_map_venues = folium.Map(location=map_center, zoom_start=10)\n",
    "\n",
    "# Add markers for each venue location\n",
    "for index, row in venues_locations.iterrows():\n",
    "    try:\n",
    "        latitude = float(row['Latitude'])\n",
    "        longitude = float(row['Longitude'])\n",
    "        folium.Marker(\n",
    "            location=[latitude, longitude],\n",
    "            popup=row['Venue'],  # Assuming 'Venue' column exists\n",
    "            icon=folium.Icon(color=\"purple\")\n",
    "        ).add_to(la_map_venues)\n",
    "    except (ValueError, TypeError):\n",
    "        print(f\"Skipping row {index} due to invalid latitude/longitude values.\")\n",
    "    except KeyError:\n",
    "      print(f\"Skipping row {index} due to missing 'Venue' column\")\n",
    "\n",
    "la_map_venues\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d8116c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "####Capacity\n",
    "\n",
    "# prompt: create a copy of venues_locations called venues locations capacity, and only return venue, latitude, longitude and capacity\n",
    "\n",
    "venues_locations_capacity = venues_locations[['Venue', 'Latitude', 'Longitude', 'Approx. Capacity']].copy()\n",
    "venues_locations_capacity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa60318",
   "metadata": {},
   "source": [
    "# Distance Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d60178",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reset the index of the cluster summary dataframe\n",
    "cluster_summary = cluster_summary.reset_index()\n",
    "#drop the sub_cluster column\n",
    "cluster_summary = cluster_summary.drop(columns=['sub_cluster'])\n",
    "cluster_summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e2a103",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a column to the venues_locations_capacity with an id Starting with V\n",
    "venues_locations_capacity['id'] = 'V' + (venues_locations_capacity.index + 1).astype(str)\n",
    "# add a column to the cluster summary with an id Starting with C\n",
    "cluster_summary['id'] = 'A' + (cluster_summary.index + 1).astype(str)\n",
    "display(venues_locations_capacity.head(5))\n",
    "cluster_summary.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b088280f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#in venues_locations_capacity, rename avg with latitude and longitude\n",
    "venues_locations_capacity = venues_locations_capacity.rename(columns={'avg_latitude': 'Latitude', 'avg_longitude': 'Longitude'})\n",
    "venues_locations_capacity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c590f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select relevant columns from both dataframes\n",
    "venues_data = venues_locations_capacity[['Latitude', 'Longitude', 'id']]\n",
    "clusters_data = cluster_summary[['avg_latitude', 'avg_longitude', 'id']].rename(\n",
    "    columns={'avg_latitude': 'Latitude', 'avg_longitude': 'Longitude'}\n",
    ")\n",
    "\n",
    "# Concatenate the dataframes vertically\n",
    "df_matrix = pd.concat([venues_data, clusters_data], ignore_index=True)\n",
    "\n",
    "# Display the merged dataframe\n",
    "df_matrix.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6869c9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import googlemaps\n",
    "\n",
    "# Replace \"YOUR_API_KEY\" with your actual Google Maps API key\n",
    "gmaps = googlemaps.Client(key=\"API_KEY\")\n",
    "\n",
    "# Function to calculate distance matrix\n",
    "def calculate_distance_matrix(origins, destinations):\n",
    "    distance_matrix = gmaps.distance_matrix(origins, destinations, mode=\"driving\")\n",
    "    return distance_matrix\n",
    "\n",
    "# Example usage\n",
    "origins = df_matrix['id'].tolist()\n",
    "destinations = df_matrix['id'].tolist()\n",
    "\n",
    "\n",
    "# Assuming df_matrix DataFrame has columns 'Latitude' and 'Longitude'\n",
    "origin_coords = df_matrix[['Latitude', 'Longitude']].values.tolist()\n",
    "dest_coords = df_matrix[['Latitude', 'Longitude']].values.tolist()\n",
    "\n",
    "distance_matrix_data = []\n",
    "for i in range(len(origins)):\n",
    "  try:\n",
    "    distance_matrix_row = []\n",
    "    for j in range(len(destinations)):\n",
    "        # Access the origin and destination coordinates as tuples\n",
    "        origin_coord = tuple(origin_coords[i])  # Convert to tuple\n",
    "        dest_coord = tuple(dest_coords[j])  # Convert to tuple\n",
    "\n",
    "        distance_result = gmaps.distance_matrix(origin_coord, dest_coord, mode=\"driving\")\n",
    "        if distance_result['rows'][0]['elements'][0]['status'] != 'OK':\n",
    "          distance_matrix_row.append(np.nan)\n",
    "        else:\n",
    "          distance_matrix_row.append(distance_result['rows'][0]['elements'][0]['distance']['value'])\n",
    "    distance_matrix_data.append(distance_matrix_row)\n",
    "  except Exception as e:\n",
    "    print(f\"Error calculating distance for row {i}: {e}\")\n",
    "    distance_matrix_data.append([np.nan] * len(destinations))\n",
    "\n",
    "\n",
    "distance_matrix_df = pd.DataFrame(distance_matrix_data, index = origins, columns = destinations)\n",
    "##distances in meters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4968593",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming df_matrix DataFrame has columns 'Latitude' and 'Longitude'\n",
    "origin_coords = df_matrix[['Latitude', 'Longitude']].values.tolist()\n",
    "dest_coords = df_matrix[['Latitude', 'Longitude']].values.tolist()\n",
    "origins = df_matrix['id'].tolist()\n",
    "destinations = df_matrix['id'].tolist()\n",
    "\n",
    "time_matrix_data = []\n",
    "for i in range(len(origins)):\n",
    "  try:\n",
    "    time_matrix_row = []\n",
    "    for j in range(len(destinations)):\n",
    "        origin_coord = tuple(origin_coords[i])  # Convert to tuple\n",
    "        dest_coord = tuple(dest_coords[j])  # Convert to tuple\n",
    "\n",
    "        distance_result = gmaps.distance_matrix(origin_coord, dest_coord, mode=\"driving\")\n",
    "        if distance_result['rows'][0]['elements'][0]['status'] != 'OK':\n",
    "          time_matrix_row.append(np.nan)\n",
    "        else:\n",
    "          time_matrix_row.append(distance_result['rows'][0]['elements'][0]['duration']['value'])\n",
    "    time_matrix_data.append(time_matrix_row)\n",
    "  except Exception as e:\n",
    "    print(f\"Error calculating time for row {i}: {e}\")\n",
    "    time_matrix_data.append([np.nan] * len(destinations))\n",
    "\n",
    "time_matrix_df = pd.DataFrame(time_matrix_data, index = origins, columns = destinations)\n",
    "# times in seconds\n",
    "\n",
    "time_matrix_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ffd874",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export the distance matrix to a csv file\n",
    "distance_matrix_df.to_csv(\"distance_matrix.csv\")\n",
    "#export the time matrix to a csv file\n",
    "time_matrix_df.to_csv(\"time_matrix.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b723e483",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export venues_locations_capacity to a csv file\n",
    "venues_locations_capacity.to_csv(\"venues.csv\", index=False)\n",
    "#export cluster_summary to a csv file\n",
    "cluster_summary.to_csv(\"clusters_central_location.csv\", index=False)\n",
    "#export merged_df to a csv file\n",
    "merged_df.to_csv(\"accomodations.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4484e9ef",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b118a87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import googlemaps\n",
    "from datetime import datetime\n",
    "\n",
    "# Initialize the Google Maps client with your API key\n",
    "gmaps = googlemaps.Client(key='KEY')\n",
    "\n",
    "# Build list of origin and destination tuples using the dataframe coordinates\n",
    "origins = accomodations_clusters[['avg_latitude', 'avg_longitude']].apply(lambda row: (row['avg_latitude'], row['avg_longitude']), axis=1).tolist()\n",
    "destinations = bus_terminals[['Latitude', 'Longitude']].apply(lambda row: (row['Latitude'], row['Longitude']), axis=1).tolist()\n",
    "\n",
    "# Request the distance matrix in batches to avoid MAX_DIMENSIONS_EXCEEDED error\n",
    "max_elements = 100\n",
    "max_destinations_per_request = 25  # Free API limit for destinations per request\n",
    "destination_batch_size = max_destinations_per_request\n",
    "origin_batch_size = max_elements // destination_batch_size\n",
    "\n",
    "# Initialize an empty results matrix\n",
    "results = [[None] * len(destinations) for _ in range(len(origins))]\n",
    "\n",
    "for i in range(0, len(origins), origin_batch_size):\n",
    "    origin_chunk = origins[i:i+origin_batch_size]\n",
    "    for j in range(0, len(destinations), destination_batch_size):\n",
    "        destination_chunk = destinations[j:j+destination_batch_size]\n",
    "        response = gmaps.distance_matrix(origin_chunk, destination_chunk, mode='driving')\n",
    "        for index_in_chunk, row in enumerate(response['rows']):\n",
    "            result_row = i + index_in_chunk\n",
    "            for dest_index, element in enumerate(row['elements']):\n",
    "                if element['status'] == 'OK':\n",
    "                    results[result_row][j+dest_index] = element['duration']['value']\n",
    "                else:\n",
    "                    results[result_row][j+dest_index] = None\n",
    "\n",
    "time_matrix = pd.DataFrame(results, index=accomodations_clusters['id'], columns=bus_terminals['id'])\n",
    "time_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e8b96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import googlemaps\n",
    "from datetime import datetime\n",
    "\n",
    "# Initialize the Google Maps client with your API key\n",
    "gmaps = googlemaps.Client(key='Key')\n",
    "\n",
    "# Build list of origin and destination tuples using the dataframe coordinates\n",
    "origins = venues[['Latitude', 'Longitude']].apply(lambda row: (row['Latitude'], row['Longitude']), axis=1).tolist()\n",
    "destinations = bus_terminals[['Latitude', 'Longitude']].apply(lambda row: (row['Latitude'], row['Longitude']), axis=1).tolist()\n",
    "\n",
    "# Request the distance matrix in batches to avoid MAX_DIMENSIONS_EXCEEDED error\n",
    "max_elements = 100\n",
    "max_destinations_per_request = 25  # Free API limit for destinations per request\n",
    "destination_batch_size = max_destinations_per_request\n",
    "origin_batch_size = max_elements // destination_batch_size\n",
    "\n",
    "# Initialize an empty results matrix\n",
    "results = [[None] * len(destinations) for _ in range(len(origins))]\n",
    "\n",
    "for i in range(0, len(origins), origin_batch_size):\n",
    "    origin_chunk = origins[i:i+origin_batch_size]\n",
    "    for j in range(0, len(destinations), destination_batch_size):\n",
    "        destination_chunk = destinations[j:j+destination_batch_size]\n",
    "        response = gmaps.distance_matrix(origin_chunk, destination_chunk, mode='driving')\n",
    "        for index_in_chunk, row in enumerate(response['rows']):\n",
    "            result_row = i + index_in_chunk\n",
    "            for dest_index, element in enumerate(row['elements']):\n",
    "                if element['status'] == 'OK':\n",
    "                    results[result_row][j+dest_index] = element['duration']['value']\n",
    "                else:\n",
    "                    results[result_row][j+dest_index] = None\n",
    "\n",
    "venues_to_bus_matrix = pd.DataFrame(results, index=venues['id'], columns=bus_terminals['id'])\n",
    "venues_to_bus_matrix"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
